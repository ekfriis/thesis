\ifx\master\undefined\input{settings/autocompile}\fi
%
\chapter{Systematics and Limit Extraction} \label{ch:systematics} 
%
In this chapter we discuss the systematic uncertainties affecting the search for
the Higgs boson and the statistical techniques used to establish an upper limit
on the Higgs~$\to\TT$ branching ratio times cross section (\xbr).  The limit can
be interpreted as the largest\footnote{At some stated level of statistical
confidence; the convention for limits in experimental high energy physics
is 95\%.} signal presence that could exist in the data and still be consistent
with the null hypothesis.  The limit on \xbr is roughly independent of the
theoretical model\footnote{Provided that the width of the Higgs bosons in the
given model is smaller than the resolution of the SVfit mass resolution.}.  In
the conclusion, we will interpret the \xbr limit result in the context of the
MSSM theory.

Proper determination of systematic uncertainties is one of the most challenging
and important components in performing a measurement correctly.  A systematic
uncertainty is the effect of the uncertainty of some ancillary measurement (or
assumption) that is used in the computation of the final result.  An instructive
example of how a systematic uncertainty can affect the final result is a
counting experiment measuring the cross section of some signal particle in the
presence of background.  The formula for the cross section times the branching
fraction is
\begin{equation}
  \sigma \times BR = \frac{N_{sig}}{\mathcal{L} \cdot \mathcal {A} \cdot
  \epsilon} = \frac{N_{obs}-N_{bkg}}{\mathcal{L} \cdot \mathcal {A} \cdot
  \epsilon}, \label{eq:CrossSectionEquation}
\end{equation}
where $N_{obs}$ is the number of events observed in data, $N_{bkg}$ is the
estimated number of background events in the observed data sample, $\mathcal{L}$
is the integrated luminosity, and $\mathcal{A} \cdot \epsilon$ is the acceptance
times efficiency of the signal.  All of the quantities in
Equation~\ref{eq:CrossSectionEquation} (with the exception of the observed
count $N_{obs}$) have some uncertainty which will effect the final measurement.
Consider a situation where the expected number of background events is
determined by fitting some sideband spectrum, and the fitted result has some
error $\delta N_{bkg}$.  The total relative effect of this error can
be obtained by error propagation
\begin{equation}
  \frac{\delta (\sigma \times BR)}{\sigma \times BR} = \frac{\partial(\sigma
  \times BR)}{\partial N_{bkg}}  \frac{1}{\sigma \times BR} \delta N_{bkg} =
  \frac{-\delta N_{bkg}} {N_{obs}-N_{bkg}}.
  \label{eq:CrossSectionEquationBkgSystematicError}
\end{equation}
It is interesting to examine
Equation~\ref{eq:CrossSectionEquationBkgSystematicError} in two scenarios.  In
the limit that $N_{obs}$ is large compared to $N_{bkg}$, the effect of the error
on the background estimate $\delta N_{bkg}$ does not affect the final result.
In contrast, in a scenario when the data is dominated by background events, the
relative error on the signal measurement due to the background estimation
approaches infinity.  The sensitivity of a measurement to a systematic
uncertainty on a parameter depends on the context in which that parameter is
used.

Experimental systematic uncertainties relevant for MSSM $H \to \TT$ search
presented in this thesis are classified in three categories: normalization
uncertainties on the signal and $Z\to\TT$ background (events with true taus),
normalization uncertainties on contributions from background with fake taus, and
shape uncertainties.  Normalization uncertainties on events with true taus are
due to lepton reconstruction, identification, isolation and trigger
efficiencies.  These terms are equivalent to the efficiency $\epsilon$ and
acceptance terms $\mathcal{A}$ of
Equation~\ref{eq:CrossSectionEquationBkgSystematicError} and affect the expected
yield of MSSM $H \to \TT$ signal and $Z \to \TT$ background events.  The
uncertainties on these effects are obtained by measuring the effect in data and
simulation, according to the procedures of Chapter~\ref{ch:corrections}, and
calculating a correction factor.  The uncertainty associated with the
measurement of the correction factor is the systematic uncertainty.  The
normalization uncertainties are assumed to be uncorrelated with the
shapes of visible and SVfit mass distributions which are
used to extract the MSSM $H \to \TT$ signal contribution in the analyzed
dataset.  Uncertainties on the shapes of the distributions are described by
``morphing'' systematics.  These are are due to uncertainties on the
momentum/energy scale of identified electrons, muons, tau and other jets in the
event.  As the SVfit mass reconstruction algorithm uses the missing transverse
energy, the shape of the SVfit distribution is sensitive to systematic
uncertainties on the overall scale \MET measurement.  The ``morphing''
systematics affect the shapes of signal as well as background contributions.
Normalization uncertainties on background contributions are estimated from the
level of agreement between data and Monte Carlo simulation in background
dominated control regions.

\section{Signal Normalization Uncertainties}

The signal normalization uncertainties are due to imperfect knowledge of how
improperly modeled effects in the simulation could affect the acceptance model,
the probability that a given signal event will pass one of the selections
(detailed in Chapter~\ref{ch:selections}).  The general procedure to quantify
these uncertainties is to measure the effect in some control region in both the
data and Monte Carlo.  The ratio of data to Monte Carlo then gives a correction
factor which is applied to the simulation.  An uncertainty on the measurement of
the effect in control region (in the data, simulation, or both) is then taken as the
systematic uncertainties.  The signal normalization uncertainties affecting this
analysis on muon trigger, reconstruction, identification and isolation
efficiencies are taken from the tag and probe analysis of $Z \to \MM$ events
presented in Section~\ref{sec:ZmumuTagAndProbe}.  A very conservative estimate
of $30\%$ is taken for the uncertainty on the tau reconstruction and
identification efficiency.  The tau identification uncertainty measurement is
discussed briefly in~\ref{sec:HadTauIdEff}.  The dependency of the Higgs boson signal
extraction on the tau identification efficiency has been studied, the result
being that uncertainties on the tau identification efficiency affect the limit
on cross--section times branching ratio for MSSM $H \to \TT$ production by a
few percent only.  An uncertainty of $11\%$ is attributed to the luminosity
measurement~\cite{LUMI}.

\section{Background Normalization Uncertainties}

Uncertainties on the normalization of background processes are obtained from the
study of background enriched control regions presented in
Chapter~\ref{ch:backgrounds}.  The main fake tau backgrounds in this analysis
are QCD multi--jet and \WpJets events.  These backgrounds are produced copiously
enough for the backgrounds to be studied in control regions dominated by a
single background process with a purity exceeding $90\%$ and an event statistics
exceeding the expected contribution of that background to the analysis by more
than one order of magnitude.  Both backgrounds are found to be well modeled by
the Monte Carlo simulation. The overall background yields used in the final fit
are measured in the background enriched control regions, and cross--checked
using the Fake--rate and Template methods. For a detailed discussion of the
measurements, see Chapter~\ref{ch:backgrounds}.  An uncertainty of $10\%$ is
attributed to the contribution of QCD and \WpJets backgrounds to the analysis.
The cross--section for \ttbarpJets production makes it difficult to select a
high purity sample of \ttbarpJets events of high event statistics.  From the
study of the $19$ events selected in the \ttbarpJets background enriched control
sample we assume an uncertainty on the \ttbarpJets background contribution in
the analysis of $30\%$.  The \ZMM background has been studied with large
statistical precision in two separate control regions, and is dominated by
events in which the reconstructed tau--jet candidate is either due to a
misidentified quark or gluon jet or due to a misidentified muon.  Good agreement
between data and Monte Carlo simulation is found in both cases.  Sizeable
uncertainties on the \ZMM background contribution arise due to the extrapolation
from the background enriched control regions to the data sample considered in
the analysis, however: the contribution of \ZMM background events to the
analysis is due to events in which one of the two muons produced in the $Z$
decay either escapes detection or fakes the signature of a hadronic tau decay.
Both cases may be difficult to model precisely in the Monte Carlo simulation.
The non--observation of a $Z$ mass peak in the mu + tau visible mass
distribution studied with the fake--rate method on the other hand sets a limit
on possible contributions from \ZMM background events.  Conservatively, we
assume an uncertainty of $100\%$ on both types of \ZMM background contributions.

\section{Shape Uncertainties}
\label{sec:ShapeUncertainties}
%
Shape uncertainties on the distributions of visible and ``full'' invariant mass
reconstructed by the SVfit algorithm are estimated by varying the electron
energy and muon momentum scale, the energy scale of tau--jets and other jets in
the event and varying the missing transverse energy in Monte Carlo simulated
events.  After each variation the complete event is re--reconstructed and passed
through the event selection.  Shifted visible and ``full'' invariant mass shapes
are obtained for each variation from the events passing all event selection
criteria.  The difference between shifted shapes and the ``nominal'' shapes
obtained from Monte Carlo simulated events with no variation of energy or
momentum scale or of the missing transverse energy applied is then taken as
shape uncertainty.

The systematic uncertainties on the muon and tau energy scales have been
provided by the muon and tau CMS Physics Object Groups and are described in
Section~\ref{sec:MuonTauMomentumScale}.  The modelling of missing transverse
energy in different types of background events has been studied in the
background enriched control regions described in Chapter~\ref{ch:backgrounds}.
No significant deviations between data and Monte Carlo simulation have been
found.  
Uncertainties due to missing transverse energy are estimated by varying
the ``clustered'' and ``unclustered'' energy scales, described in
Section~\ref{sec:ZRecoilCorr} and recomputing the total \MET.
%Uncertainties due to missing transverse energy are estimated by varying
%parameters of the $Z$--recoil corrections within the uncertainties obtained when
%fitting (see Section~\ref{sec:ZRecoilCorr}) the $Z$--recoil correction factor
%parameters in simulated $Z \to \MM$ events versus $Z \to \MM$ events selected in
%data.

\section{Theory Uncertainties}

The signal and background normalization as well as the shape uncertainties are
all experimental uncertainties in nature.  Additional theoretical uncertainties
arise from imprecise knowledge of parton--distribution functions (PDFs) and of
the exact dependency of signal cross--sections and branching ratios on
$\tan\beta$ and \ma.  The PDFs describe how the energy of the protons is shared
between the quarks and gluons.  Since the longitudinal boost of the hard
collisions depends on the PDFs, the signal acceptance is sensitive to errors in
the PDFs.  The uncertainties on the signal acceptance due to PDF uncertainties
are estimated using tools developed by the CMS Electroweak working
group~\cite{CMS_EWK_pdfUncertaintyTools}.  The acceptance is computed with
respect to MSSM $H \to \TT$ decays that have muons of $\pt^{\mu} > 15$~\GeVc
and $\left| \eta_{\mu} \right| < 2.1$, jets produced in hadronic tau decays with
visible $\pt^{vis} > 20$~\GeVc and $\left| \eta_{vis} \right| < 2.3$ on
generator level.  Acceptance values are computed for the central value and $44$
eigenvectors of the CTEQ66 PDF set~\cite{CTEQpdfSet}.  The systematic
uncertainty on the signal acceptance is computed following the PDF4LHC
recommendations~\cite{pdfAccSys01,pdfAccSys02}.

The effect of Monte Carlo normalization, shape and theory uncertainties on the
signal efficiency times acceptance is summarized in
Table~\ref{tab:ExpUncertainties}.

\begin{table}[t]
\begin{center}
\tablesize
\begin{tabular}{|l|c|}
\hline
Source & Effect \\
\hline
\hline
\multicolumn{2}{|c|}{Normalization uncertainties} \\
\hline
Trigger                         & $0.981 \pm 0.006$ \\
Muon identification             & $1.001 \pm 0.001$ \\
Muon isolation                  & $0.984 \pm 0.006$ \\
Tau--jet identification         & $1.00  \pm 0.30$ \\
\hline
\hline
\multicolumn{2}{|c|}{Shape uncertainties} \\
\hline
Muon momentum scale             & $\ll 1\%$ \\
Tau--jet energy scale           & $1 - 4\%^{1}$ \\
Jet energy scale (JES)          & $< 1\%^{2}$ \\
$\MET$ ($Z$--recoil correction) & $1\%$ \\
\hline
\hline
\multicolumn{2}{|c|}{Theory uncertainties} \\
\hline
PDF & $2\%^{3}$ \\
\hline
\end{tabular}
\end{center}
$^{1}$ decreasing with $m_{A}$ \\
$^{2}$ number quoted for $gg \to A/H$ and $b\bar{b} \to A/H$ sample as a whole; \\
\hspace{5mm} in the subsample of events with b--tagged jets the effect of the JES uncertainty is $4\%$ \\
$^{3}$ with small dependence on $m_{A}$ \\
\begin{center}
\caption[Effect of normalization uncertainties on signal efficiency times
acceptance]{\captiontext Effect of normalization uncertainties on the $gg \to
A/H$ and $b\bar{b} \to A/H$ signal efficiency times acceptance.}
\label{tab:ExpUncertainties}
\end{center}
\end{table}
%
\section{Limit Extraction Method}
\label{sec:statmethod}
%
The search for a new signal is performed by examining the observed distribution
of the reconstructed di--tau mass $m_{\tau\tau}$ as reconstructed by the SVfit
algorithm.  An ``bump'' in this spectrum would indicate the presence of a new
particle.  To make a statement about the presence of a bump with confidence, the
shape under a potential bump must be well described.  The background shape is
decomposed into the combination of shapes from the difference background
sources, which we refer to here as ``templates.'' The data and the templates for
each background distribution is binned in the observable $m_{\tau\tau}$
variable.  The normalization of each template represents the total yield
expected for that source.

We can then define a likelihood for any configuration of our templates given the
observed data.  The likelihood is a ``binned Poisson likelihood,'' which is
defined as the product of the Poisson probability in each bin.  The Poisson
probability $P(n|\mu)$ is the probability to observe $n$ events given an
expectation of $\mu$ events.  The Poisson probability is given by
the expression 
\begin{equation}
  P(k|\mu) = \frac{\mu^k e^{-k}}{k!}.
  \label{eq:PoissonProb}
\end{equation}
The total likelihood for observed data given some configuration of templates is
then simply the product of the Poisson probabilities (Equation~\ref{eq:PoissonProb}) 
in each of the $N_{bin}$ bins: 
\begin{eqnarray}
  {\cal L} = \prod_{i=1}^{N_{bin}} \frac{\mu_i^{n_i} e^{-\mu_i}}{n_i!},
  \label{eq:CoreLikelihood}
\end{eqnarray}
where the expected number of events $\mu_i$ in the bin $i$ is the sum of the number
of events from all sources
\begin{eqnarray}
  \mu_i = \sum_{j=1}^{N_{source}} \mu_{ji}. \nonumber
\end{eqnarray}
The number of expected events in a source, in turn, can be written
\begin{eqnarray}
  \mu_{ji} = L \sigma_j \epsilon_{ji}
  \label{eq:ShapeParameterization}
\end{eqnarray}
where $L$ is the integrated luminosity, $\sigma_j$ is the cross section
for source $j$, and $\epsilon_{ji}$ is the efficiency for source $j$ in bin
$i$.  

We incorporate the systematic uncertainties of the analysis  by introducing a
set of ``nuisance parameters'' $\vec \beta$ into the likelihood function.  As
the name suggests, we are not interested in the actual value of the nuisance
parameters.  Each nuisance parameter parameterizes some phenomenon in the
analysis.  The shape templates (which can be defined purely in terms of
$\sigma_j$ and $\epsilon_{ji}$ in Equation~\ref{eq:ShapeParameterization}) are
now interpreted as functions of the set of nuisance parameters $\vec \beta$.
Existing knowledge about the value of the nuisance parameter is introduced by
extending the likelihood function with a constraint ${\cal G}(\vec \beta)$ 
that expresses the knowledge about the nuisance parameters. 
The templates $\mu_{ji}$ can depend on the nuisance parameters in two ways.
Normalization uncertainties introduce multiplicative nuisance factors on the
yield of some (sub)set of the sources defined by
Equation~\ref{eq:ShapeParameterization}.  As an example, consider a simplified
situation where there are only two sources: \ZTT and \WpJets events.  Both 
sources are sensitive to the efficiency of the muon trigger.  Only the \ZTT
sample is sensitive to the efficiency of the hadronic tau identification
algorithm, as the fake--rate in \WpJets is measured in data.  We would then
introduce two multiplicative nuisance parameters, $\beta_\mu$ and $\beta_\tau$,
which respectively correspond to the two uncertainties.  The expected number of
events in the \mbox{$i$th} bin is then given by a modified form of
Equation~\ref{eq:ShapeParameterization}
\begin{eqnarray}
  \mu_i = L \beta_\mu \sigma^W \epsilon^W_{i} 
  + L \beta_\mu \beta_\tau  \sigma^Z \epsilon^Z_i.
  \label{eq:ShapeParameterizationWithNuisance}
\end{eqnarray}
From Equation~\ref{eq:ShapeParameterizationWithNuisance}, we can see that
$\beta_\mu$ affects both sources but $\beta_\tau$ only
affects the \ZTT source.

The shape uncertainties discussed in Section~\ref{sec:ShapeUncertainties} are
incorporated using a technique called ``vertical template morphing.''  For each
source, different templates are created for three different values of the
morphing parameter, corresponding to -1, 0, and +1 standard deviation shifts in
the nuisance parameter. To determine the number of expected events in the $i$th
bin as a function of the morphing parameter, we interpret quadratically between
the $i$th bin values of the three templates, and extrapolate linearly beyond
them.
The overall likelihood then, including nuisance parameters, can be written as
\begin{equation}
  {\cal L} = \prod_{i=1}^{N_{bin}} 
  \frac{[\mu_i(\vec \beta)]^{n_i} e^{-\mu_i(\vec \beta)}}{n_i!} 
  \times \prod_{m=1}^{N_\beta}{\cal G}(\beta_m).
  \label{eq:LikelihoodNuisance}
\end{equation}
In Equation~\ref{eq:LikelihoodNuisance}, we have introduced the assumption that
the nuisance parameters are uncorrelated.  

We wish to determine if the data is compatible with a new signal.  To test for
the presence of a bump, we examine the likelihood as a function of the signal
cross-section.  If the presence of a signal is unambiguous, one can simply
determine the likelihood as a function of the cross-section.  If the signal is
known to be non--zero, confidence intervals about the maximum likelihood can be
constructed by examining the change in the logarithm $2\Delta \ln {\cal L}$ of
the likelihood.  The Frequentist \fixme{Did I mix this up?}interpretation of the
confidence level $(1-\alpha)$ is that if the experimented were repeated $N$
times, the interval corresponding to $(1-\alpha)$ would fail ``cover'' the
\emph{true} value of the observable in $\alpha \times N$ of the experiments.
The correspondence between confidence levels and intervals in $2\Delta \ln {\cal
L}$ is given in Table~\ref{tab:ConfidenceIntervals}.
\begin{table}
  \begin{center}
    \begin{tabular}{c|ccc} 
      $(1-\alpha)\%$ & $m = 1$ & $m = 2$ & $m = 3$ \\
      \hline
      68.27 & 1.00 & 2.30 & 3.53 \\
      90.00 & 2.71 & 4.61 & 6.25 \\
      95.00 & 3.84 & 5.99 & 7.82 \\
      99.00 & 6.63 & 9.21 & 11.34 \\
    \end{tabular}
    \caption[Correspondence of confidence levels and $2\Delta \ln {\cal L}$
    intervals]{Correspondence between a confidence level defined by
    $(1-\alpha)\%$ and the corresponding interval in $2\Delta \ln {\cal L}$.
    The $2\Delta \ln {\cal L}$ interval is different depending the number $m$ of
    parameters which are being simultaneously estimated.}
    \label{tab:ConfidenceIntervals}
  \end{center}
\end{table}
Since the mass of a potential Higgs boson is
unknown, we repeat this process for different signal masses.  If there is not a
significant signal, we can set upper bounds on the signal cross-section using
one of several methods, which will be discussed below.

At this point, the likelihood still depends on the nuisance parameters.  There
are two methods of removing the dependence on the nuisance parameters,
``marginalization'' and ``profiling.'' Marginalization is the process of
integrating the likelihood of Equation~\ref{eq:LikelihoodNuisance} over the
entire range of all nuisance parameters.  This effectively averages the effect
of the different uncertainties into the marginalized likelihood function.  The
profiling method maximizes the likelihood in terms of the nuisance
parameters.  In the profiling method, the ``profile likelihood'' is created by
maximizing the likelihood with respect to all of the nuisance parameters.  One
way to interpret the profiling method is that the values of nuisance parameters
are being measured \emph{in situ}, constrained by the observed data.  While not
a strictly Bayesian treatment, the profile likelihood method has been shown to
give almost identical results to marginalization.

An interesting situation relating to the profiling of the nuisance parameters
arises in this analysis in the context of the hadronic tau identification
uncertainty.  As discussed in Chapter~\ref{ch:corrections}, the
Higgs--insensitive measurement of the tau identification efficiency has a high
uncertainty of 30\%.  If the bump at the \ZTT resonance can be considered to
free of contributions from a Higgs boson, the tau identification efficiency can be measured to a much greater
precision, approximately 7\%~\cite{CMS-PAS-EWK-10-013, CMS-PAS-TAU-11-001}.
When profiling the likelihood (as a function of cross section) for a Higgs boson with
a mass $\ma > m_Z$, the likelihood contains the information that there is no
Higgs boson
contribution to the $Z$ resonance.  This fact enables the profiling
process to constrain the systematic uncertainty on the tau identification
efficiency to the 7\% level using the size of the $Z$ resonance.  Conversely,
when the likelihood is evaluating the likelihood of the presence of a Higgs
boson
signal with $\ma = m_Z$, the likelihood cannot distinguish between a potential
presence of Higgs boson events in the $Z$ bump or a systematic undershoot of the tau
identification efficiency in the simulation.  In this case the profiling
processing has no power to constrain the systematic to a value lower than the
initial 30\% value.

In the absence of the signal, or even in the presence of one, we can
determine a upper $95\%$ CL bound on the cross-section of the signal using
the profile likelihood.  In one method we simply use Bayes' Theorem to
convert the likelihood to a posterior density in the signal
cross-section, and integrate to find the point below which 95\% of the
probability lies.  The Bayesian posterior PDF is computed as
\begin{equation}
  {\cal P}\left( \sigma_H \vert \overline x, \, m_H \right) = 
  \frac
  {{\cal L}\left( \overline x \vert \sigma_{H} \, m_H \right) {\cal P}\left( \sigma_H \right)}
  {\int {\cal L}\left( \overline x \vert \sigma'_H \, m_H\right) {\cal P}\left(
  \sigma'_H \right) d\sigma'_H }.
  \label{eq:BayesianPosterior}
\end{equation}
The left hand side of Equation~\ref{eq:BayesianPosterior} represents the
probability density for a given signal cross section, given the observed data
$\overline x$ and the assumed value $m_H$ for the Higgs boson mass.  We refer to this
method as the ``Bayesian'' method of setting limits.

In the other method, which is referred to as the ``Delta Log Likelihood'' (DLL)
method, we compute two likelihoods.  The first likelihood is computed for the
``null hypothesis'' case.  The likelihood is profiled (maximized) assuming that
no signal is present.  We then construct the profile likelihood for increasing
values of the signal yield.  The upper limit is achieved when the logarithm of
the profile likelihood is 1.92 units less than the value of the null hypothesis
profile likelihood.  

In general the limits computed by the Bayesian and DLL methods are similar.
However, the effect of upward or downward statistical fluctuations in the
observed data affect the two methods in different ways.  When the data
fluctuates low, the DLL method will produce a more stringent limit than the
DLL method.  When the data fluctuates high, creating an apparent signal, the
Bayesian method will (correctly) set a more stringent limit.  
%In this thesis we report the results of both methods.
%
%we find that point where the logarithm of the
%profile likelihood is 1.92 units below the value of the likelihood at
%zero signal cross-section. This gives similar limits to the previous
%method, which tend to be more stringent when there is a negative
%fluctuation or no fluctuation in the apparent signal, but less
%stringent than the Bayesian limits when there is an upward fluctuation
%giving an apparent signal.  The complete examination of the coverage
%properties of these two methods is beyond the scope of this note. We
%report the results of both prescriptions below.

In the results presented below we use nuisance parameters corresponding
to the systematic errors summarized in Table~\ref{tab-sys}.
\begin{table}
  \begin{center}
    \begin{tabular}{|l|c|c|} \hline
  Source                 &       Method      &   Magnitude  \\ \hline
Muon ID/trigger          &  Multiplicative   &    20\%      \\
$Z$ cross section        &  Multiplicative   &     5\%      \\
Jet to $\tau$ fake rate  &  Multiplicative   &    20\%      \\
$\mu\to\tau$ fake rate   &  Multiplicative   &   100\%      \\
$W+$jets cross section   &  Multiplicative   &    10\%      \\
$t\bar{t}$ cross section &  Multiplicative   &    40\%      \\
integrated luminosity    &  Multiplicative   &    11\%      \\
Tau energy scale         &  Morphing         &     3\%      \\
Jet energy scale      &  Morphing         &     3\%      \\
Unclustered energy scale      &  Morphing         &     1\%      \\
Z--recoil correction      &  Morphing         &     -      \\
Muon $p_T$ scale         &  Morphing         &   neg.       \\
EM energy scale          &  Morphing         &   neg.       \\ \hline
    \end{tabular}
   \end{center}
  \caption[Summary of systematic uncertainties]{Summary of systematic
  uncertainties represented by nuisance parameters in the likelihood, their
  representation method and magnitudes.  The Z--recoil correction factor is
  varied within the uncertainty on the measurement described in
  Chapter~\ref{ch:corrections}. \label{tab-sys}}
\end{table}


\ifx\master\undefined\input{settings/autocompile}\fi
