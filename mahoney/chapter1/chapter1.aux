\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\@writefile{toc}{\contentsline {chapter}{\numberline {1}Computational Mechanics}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:compmech}{{1}{1}{Computational Mechanics\relax }{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Philosophy}{1}{section.1.1}}
\newlabel{sec:philosophy}{{1.1}{1}{Philosophy\relax }{section.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}\unhbox \voidb@x \hbox {$\epsilon $-machine}\ Prelude}{2}{subsection.1.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Our Domain: Processes}{3}{section.1.2}}
\citation{Crut92c}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}\unhbox \voidb@x \hbox {$\epsilon $-machines}}{5}{section.1.3}}
\citation{Crut98d}
\citation{Crut88a}
\citation{Crut98d}
\citation{Crut98d}
\citation{Crut88a}
\citation{Crut98d}
\citation{Crut88a}
\citation{Shal98a}
\citation{Hopc79}
\citation{Ephr02a}
\newlabel{shield}{{1.1}{7}{\eMs \relax }{equation.1.3.1}{}}
\newlabel{shield}{{1.2}{7}{\eMs \relax }{equation.1.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}First Examples}{8}{section.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}IID}{8}{subsection.1.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces IID binary process.}}{8}{figure.1.1}}
\newlabel{fig:IID_eM}{{1.1}{8}{IID binary process}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Entropy and Entropy Rate}{8}{subsection.1.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces (Left) Block entropy curve for all binary IID processes ($\qopname  \relax m{Pr}( {X} =0) = p$). Each is simply a line through the origin with slope determined by $p$. Since there is a symmetry in the class about $p=0.5$, only on half of the $p$ values are illustrated. (Right) Finite length entropy rate approximations. Colorbar indicates the probability, $p$, that is varied to obtain different members of a process family. The particular probability, $p$, refers to the variable in Fig.\nobreakspace  {}\ref  {fig:IID_eM}.}}{10}{figure.1.2}}
\newlabel{fig:BE_IID_all}{{1.2}{10}{(Left) Block entropy curve for all binary IID processes ($\Prob (\MeasSymbol =0) = p$). Each is simply a line through the origin with slope determined by $p$. Since there is a symmetry in the class about $p=0.5$, only on half of the $p$ values are illustrated. (Right) Finite length entropy rate approximations. Colorbar indicates the probability, $p$, that is varied to obtain different members of a process family. The particular probability, $p$, refers to the variable in Fig.~\ref {fig:IID_eM}}{figure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces The Golden Mean Process is one that disallows consecutive zeros. After a zero [one] is seen, the process is in causal state $B$ $[A]$.}}{11}{figure.1.3}}
\newlabel{fig:GM_eM}{{1.3}{11}{The Golden Mean Process is one that disallows consecutive zeros. After a zero [one] is seen, the process is in causal state $B$ $[A]$}{figure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces (Left) Block entropy curves for all Golden Mean processes. Each is linear for $L\ge 1$. (Right) Each finite length entropy rate estimate reaches its asymptotic value $h_\mu $, at $L=2$. This indicates that the additional uncertainty in the $L=2$ blocks, beyond the $L=1$ blocks, is already $h_\mu $. This implies that the minimum correlation length required for maximal prediction ability is $L=1$. That is, the Golden Mean is an order\nobreakdash -$1$ Markov process.}}{12}{figure.1.4}}
\newlabel{fig:BE_GM_all}{{1.4}{12}{(Left) Block entropy curves for all Golden Mean processes. Each is linear for $L\ge 1$. (Right) Each finite length entropy rate estimate reaches its asymptotic value $\hmu $, at $L=2$. This indicates that the additional uncertainty in the $L=2$ blocks, beyond the $L=1$ blocks, is already $\hmu $. This implies that the minimum correlation length required for maximal prediction ability is $L=1$. That is, the Golden Mean is an \order {1} Markov process}{figure.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Nonunifilar presentation of the Golden Mean Process. The entropy rate of the process is \emph  {not} simply the weighted average of the entropies of symbols emitted after visiting each state.}}{13}{figure.1.5}}
\newlabel{fig:GM_nonunif}{{1.5}{13}{Nonunifilar presentation of the Golden Mean Process. The entropy rate of the process is \emph {not} simply the weighted average of the entropies of symbols emitted after visiting each state}{figure.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Statistical Complexity}{13}{section.1.5}}
\citation{Crut01a}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Excess Entropy}{15}{section.1.6}}
\citation{Fras90b}
\citation{Casd91a}
\citation{Spro03a}
\citation{Kant06a}
\citation{Arno96}
\citation{Crut97a}
\citation{Feld98b}
\citation{Tono94a}
\citation{Bial00a}
\citation{Ebel94c}
\citation{Debo08a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces A stochastic process can be viewed as a communication channel. The data in the past is the input to the channel. The channel itself is the dynamical system, or \unhbox \voidb@x \hbox {$\epsilon $-machine}, which transmits information to the future. The total information transmitted from past to future is equal to the excess entropy.}}{17}{figure.1.6}}
\newlabel{fig:CommunicationChannel}{{1.6}{17}{A stochastic process can be viewed as a communication channel. The data in the past is the input to the channel. The channel itself is the dynamical system, or \eM , which transmits information to the future. The total information transmitted from past to future is equal to the excess entropy}{figure.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces This I-diagram highlights the role of excess entropy as the mutual information between past and future data.}}{17}{figure.1.7}}
\newlabel{fig:2var_IXYisE}{{1.7}{17}{This I-diagram highlights the role of excess entropy as the mutual information between past and future data}{figure.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces The generic relation among the past, future and a state, $ \mathcal  {R} $ includes 15 nontrivial information quantities. Demanding that the state involved is a causal state effects 4 of these quantities. The green area (which corresponds to two information atoms) is zero because the causal state is a single-valued function of the past. The purple area is zero because causal states are prescient. The orange area is not zero, but is the minimum value possible, given that green and purple are zero.}}{18}{figure.1.8}}
\newlabel{fig:3var_PastFutureRcolored}{{1.8}{18}{The generic relation among the past, future and a state, $\AlternateState $ includes 15 nontrivial information quantities. Demanding that the state involved is a causal state effects 4 of these quantities. The green area (which corresponds to two information atoms) is zero because the causal state is a single-valued function of the past. The purple area is zero because causal states are prescient. The orange area is not zero, but is the minimum value possible, given that green and purple are zero}{figure.1.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Estimation of Excess Entropy}{19}{section.1.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}Example of Sharp Convergence : Order-3 Markov}{19}{subsection.1.7.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces A process with 8 causal states. Since each state has two outgoing transitions, each of which has one free parameter, we suppress the probabilities here. The reader may verify that this is the structure of an order-3 Markov process---any 3 symbols will uniquely define a state (the converse happens to also be true in this instance).}}{20}{figure.1.9}}
\newlabel{fig:random_order3}{{1.9}{20}{A process with 8 causal states. Since each state has two outgoing transitions, each of which has one free parameter, we suppress the probabilities here. The reader may verify that this is the structure of an order-3 Markov process---any 3 symbols will uniquely define a state (the converse happens to also be true in this instance)}{figure.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.2}Example of Slow Convergence : Even Process}{20}{subsection.1.7.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Excess entropy estimates for 50 instances of full order-3 Markov chains (see Fig.\nobreakspace  {}\ref  {fig:random_order3} for the topology). Notice that the estimates become extremely good (actually exact) at $L=3$. This is a consequence of the process being finite order Markov.}}{21}{figure.1.10}}
\newlabel{fig:E_random_order3}{{1.10}{21}{Excess entropy estimates for 50 instances of full order-3 Markov chains (see Fig.~\ref {fig:random_order3} for the topology). Notice that the estimates become extremely good (actually exact) at $L=3$. This is a consequence of the process being finite order Markov}{figure.1.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces The Even Process requires that all blocks of uninterrupted ones, with zeros on either side, be even in length.}}{21}{figure.1.11}}
\newlabel{fig:even_feM_gr}{{1.11}{21}{The Even Process requires that all blocks of uninterrupted ones, with zeros on either side, be even in length}{figure.1.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces The above table illustrates that for the Even Process, for any length $N$, there exists a word (all ones) such that prediction based on that word alone is different than prediction based on that word knowing that the previous symbol is 0. Notice that the optimal probabilities, those predicted after the block of ones is begun by a zero, are the same as those predicted by the appropriate induced causal state.}}{22}{table.1.1}}
\newlabel{tab:EvenProcessProbs}{{1.1}{22}{The above table illustrates that for the Even Process, for any length $N$, there exists a word (all ones) such that prediction based on that word alone is different than prediction based on that word knowing that the previous symbol is 0. Notice that the optimal probabilities, those predicted after the block of ones is begun by a zero, are the same as those predicted by the appropriate induced causal state}{table.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces The non-Markovianness of the Even Process leads to some members of the family having very slow convergence. (Left) Relative errors in the excess entropy estimates show that even considering correlation lengths up to 10 is grossly inadequate for a large collection of processes. (Right) Relative error of entropy rate estimates are very slow to approach zero for members on the blue end of the spectrum. This process serves as a key motivating example in the search for analytic forms for {\bf  E}.}}{23}{figure.1.12}}
\newlabel{fig:E_hmu_Even_all_reldiff}{{1.12}{23}{The non-Markovianness of the Even Process leads to some members of the family having very slow convergence. (Left) Relative errors in the excess entropy estimates show that even considering correlation lengths up to 10 is grossly inadequate for a large collection of processes. (Right) Relative error of entropy rate estimates are very slow to approach zero for members on the blue end of the spectrum. This process serves as a key motivating example in the search for analytic forms for \EE }{figure.1.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces Excess entropy estimates for the Even Process without access to the actual limit {\bf  E}. Its estimates increase in a very slow manner making claims about convergence, except for very trivial ones, difficult.}}{24}{figure.1.13}}
\newlabel{fig:E_Even_all}{{1.13}{24}{Excess entropy estimates for the Even Process without access to the actual limit \EE . Its estimates increase in a very slow manner making claims about convergence, except for very trivial ones, difficult}{figure.1.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Crypticity and Cryptic Order}{24}{section.1.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.1}Crypticity}{24}{subsection.1.8.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.2}Cryptic Order}{25}{subsection.1.8.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces This highlights the crypticity $\chi $ in orange as the difference between the state information $C_\mu $ and the predictive information ${\bf  E}$. In this sense, crypticity can be thought of as `modeling overhead'.}}{26}{figure.1.14}}
\newlabel{fig:3var_PastFutureS}{{1.14}{26}{This highlights the crypticity $\PC $ in orange as the difference between the state information $\Cmu $ and the predictive information $\EE $. In this sense, crypticity can be thought of as `modeling overhead'}{figure.1.14}{}}
\bibstyle{../bibliography/expanded}
\bibdata{../bibliography/references}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces An illustration of a process which is order-4 Markov. The past $H[\Past ]$ is shown as being stratified in the standard way. We can see that conditioning on the past 4 variables reduces as much uncertainty in the future as does conditioning on the entire past. Conditioning on only the past 3 variables, however, neglects the upper tip of the mutual information, ${\bf  E}=I[\Past ;\Future ]$.}}{27}{figure.1.15}}
\newlabel{fig:FoliationMarkov}{{1.15}{27}{An illustration of a process which is order-4 Markov. The past $H[\protect \Past ]$ is shown as being stratified in the standard way. We can see that conditioning on the past 4 variables reduces as much uncertainty in the future as does conditioning on the entire past. Conditioning on only the past 3 variables, however, neglects the upper tip of the mutual information, $\EE =I[\protect \Past ;\protect \Future ]$}{figure.1.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces This is another illustration of an order-4 Markov process. The causal state has been added to the diagram and the boundaries made a little more curvy to anticipate future I-diagrams. Notice that in addition to the length 4 statistics being sufficient for capturing ${\bf  E}$, the same is true for capturing $\chi $ which is the remainder of $C_\mu $. In contrast, the length 3 statistics are insufficient for both ${\bf  E}$ and $\chi $. Being insufficient for ${\bf  E}$ is why the process is order-4 Markov. Being insufficient for $\chi $ is why the process is order-4 cryptic.}}{27}{figure.1.16}}
\newlabel{fig:FoliationCrypticReqk}{{1.16}{27}{This is another illustration of an order-4 Markov process. The causal state has been added to the diagram and the boundaries made a little more curvy to anticipate future I-diagrams. Notice that in addition to the length 4 statistics being sufficient for capturing $\EE $, the same is true for capturing $\PC $ which is the remainder of $\Cmu $. In contrast, the length 3 statistics are insufficient for both $\EE $ and $\PC $. Being insufficient for $\EE $ is why the process is order-4 Markov. Being insufficient for $\PC $ is why the process is order-4 cryptic}{figure.1.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces An illustration of a process with differing cryptic and Markov orders. The Markov order is 4; this is the first history length which contains all of the predictive information. Notice that the length 3 history curves back again missing a portion of ${\bf  E}$. The cryptic order is 3 because although the length 3 history misses some portion of ${\bf  E}$, it does determine the causal state conditioned on the future. Note that $H[ {X} _{-3}^3]$ is labeled twice for clarity.}}{28}{figure.1.17}}
\newlabel{fig:FoliationCryptic}{{1.17}{28}{An illustration of a process with differing cryptic and Markov orders. The Markov order is 4; this is the first history length which contains all of the predictive information. Notice that the length 3 history curves back again missing a portion of $\EE $. The cryptic order is 3 because although the length 3 history misses some portion of $\EE $, it does determine the causal state conditioned on the future. Note that $H[\MeasSymbol _{-3}^3]$ is labeled twice for clarity}{figure.1.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.18}{\ignorespaces The Markov and cryptic orders may differ by more than one. This is an instance where the Markov order is 4, yet the cryptic order is 2. Two entropies are labeled twice for clarity.}}{28}{figure.1.18}}
\newlabel{fig:FoliationCrypticRgegek}{{1.18}{28}{The Markov and cryptic orders may differ by more than one. This is an instance where the Markov order is 4, yet the cryptic order is 2. Two entropies are labeled twice for clarity}{figure.1.18}{}}
