\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\newlabel{ch:0}{{}{\textit  {vi}}{How To Read This Thesis\relax }{chapter*.3}{}}
\citation{Crut08a}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Computational Mechanics}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:compmech}{{1}{1}{Computational Mechanics\relax }{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Philosophy}{1}{section.1.1}}
\newlabel{sec:philosophy}{{1.1}{1}{Philosophy\relax }{section.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}\unhbox \voidb@x \hbox {$\epsilon $-machine}\ Prelude}{2}{subsection.1.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Our Domain: Processes}{3}{section.1.2}}
\citation{Crut92c}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}\unhbox \voidb@x \hbox {$\epsilon $-machines}}{5}{section.1.3}}
\citation{Crut98d}
\citation{Crut88a}
\citation{Crut98d}
\citation{Crut98d}
\citation{Crut88a}
\citation{Crut98d}
\citation{Crut88a}
\citation{Shal98a}
\citation{Hopc79}
\citation{Ephr02a}
\newlabel{shield}{{1.1}{7}{\eMs \relax }{equation.1.3.1}{}}
\newlabel{shield}{{1.2}{7}{\eMs \relax }{equation.1.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}First Examples}{8}{section.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}IID}{8}{subsection.1.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces IID binary process.}}{8}{figure.1.1}}
\newlabel{fig:IID_eM}{{1.1}{8}{IID binary process}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Entropy and Entropy Rate}{8}{subsection.1.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces (Left) Block entropy curve for all binary IID processes ($\qopname  \relax m{Pr}( {X} =0) = p$). Each is simply a line through the origin with slope determined by $p$. Since there is a symmetry in the class about $p=0.5$, only on half of the $p$ values are illustrated. (Right) Finite length entropy rate approximations. Colorbar indicates the probability, $p$, that is varied to obtain different members of a process family. The particular probability, $p$, refers to the variable in Fig.\nobreakspace  {}\ref  {fig:IID_eM}.}}{10}{figure.1.2}}
\newlabel{fig:BE_IID_all}{{1.2}{10}{(Left) Block entropy curve for all binary IID processes ($\Prob (\MeasSymbol =0) = p$). Each is simply a line through the origin with slope determined by $p$. Since there is a symmetry in the class about $p=0.5$, only on half of the $p$ values are illustrated. (Right) Finite length entropy rate approximations. Colorbar indicates the probability, $p$, that is varied to obtain different members of a process family. The particular probability, $p$, refers to the variable in Fig.~\ref {fig:IID_eM}}{figure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces The Golden Mean Process is one that disallows consecutive zeros. After a zero [one] is seen, the process is in causal state $B$ $[A]$.}}{11}{figure.1.3}}
\newlabel{fig:GM_eM}{{1.3}{11}{The Golden Mean Process is one that disallows consecutive zeros. After a zero [one] is seen, the process is in causal state $B$ $[A]$}{figure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces (Left) Block entropy curves for all Golden Mean processes. Each is linear for $L\ge 1$. (Right) Each finite length entropy rate estimate reaches its asymptotic value $h_\mu $, at $L=2$. This indicates that the additional uncertainty in the $L=2$ blocks, beyond the $L=1$ blocks, is already $h_\mu $. This implies that the minimum correlation length required for maximal prediction ability is $L=1$. That is, the Golden Mean is an order\nobreakdash -$1$ Markov process.}}{12}{figure.1.4}}
\newlabel{fig:BE_GM_all}{{1.4}{12}{(Left) Block entropy curves for all Golden Mean processes. Each is linear for $L\ge 1$. (Right) Each finite length entropy rate estimate reaches its asymptotic value $\hmu $, at $L=2$. This indicates that the additional uncertainty in the $L=2$ blocks, beyond the $L=1$ blocks, is already $\hmu $. This implies that the minimum correlation length required for maximal prediction ability is $L=1$. That is, the Golden Mean is an \order {1} Markov process}{figure.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Nonunifilar presentation of the Golden Mean Process. The entropy rate of the process is \emph  {not} simply the weighted average of the entropies of symbols emitted after visiting each state.}}{13}{figure.1.5}}
\newlabel{fig:GM_nonunif}{{1.5}{13}{Nonunifilar presentation of the Golden Mean Process. The entropy rate of the process is \emph {not} simply the weighted average of the entropies of symbols emitted after visiting each state}{figure.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Statistical Complexity}{13}{section.1.5}}
\citation{Crut01a}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Excess Entropy}{15}{section.1.6}}
\citation{Fras90b}
\citation{Casd91a}
\citation{Spro03a}
\citation{Kant06a}
\citation{Arno96}
\citation{Crut97a}
\citation{Feld98b}
\citation{Tono94a}
\citation{Bial00a}
\citation{Ebel94c}
\citation{Debo08a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces A stochastic process can be viewed as a communication channel. The data in the past is the input to the channel. The channel itself is the dynamical system, or \unhbox \voidb@x \hbox {$\epsilon $-machine}, which transmits information to the future. The total information transmitted from past to future is equal to the excess entropy.}}{17}{figure.1.6}}
\newlabel{fig:CommunicationChannel}{{1.6}{17}{A stochastic process can be viewed as a communication channel. The data in the past is the input to the channel. The channel itself is the dynamical system, or \eM , which transmits information to the future. The total information transmitted from past to future is equal to the excess entropy}{figure.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces This I-diagram highlights the role of excess entropy as the mutual information between past and future data.}}{17}{figure.1.7}}
\newlabel{fig:2var_IXYisE}{{1.7}{17}{This I-diagram highlights the role of excess entropy as the mutual information between past and future data}{figure.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces The generic relation among the past, future and a state, $ \mathcal  {R} $ includes 15 nontrivial information quantities. Demanding that the state involved is a causal state effects 4 of these quantities. The green area (which corresponds to two information atoms) is zero because the causal state is a single-valued function of the past. The purple area is zero because causal states are prescient. The orange area is not zero, but is the minimum value possible, given that green and purple are zero.}}{18}{figure.1.8}}
\newlabel{fig:3var_PastFutureRcolored}{{1.8}{18}{The generic relation among the past, future and a state, $\AlternateState $ includes 15 nontrivial information quantities. Demanding that the state involved is a causal state effects 4 of these quantities. The green area (which corresponds to two information atoms) is zero because the causal state is a single-valued function of the past. The purple area is zero because causal states are prescient. The orange area is not zero, but is the minimum value possible, given that green and purple are zero}{figure.1.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Estimation of Excess Entropy}{19}{section.1.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}Example of Sharp Convergence : Order-3 Markov}{19}{subsection.1.7.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces A process with 8 causal states. Since each state has two outgoing transitions, each of which has one free parameter, we suppress the probabilities here. The reader may verify that this is the structure of an order-3 Markov process---any 3 symbols will uniquely define a state (the converse happens to also be true in this instance).}}{20}{figure.1.9}}
\newlabel{fig:random_order3}{{1.9}{20}{A process with 8 causal states. Since each state has two outgoing transitions, each of which has one free parameter, we suppress the probabilities here. The reader may verify that this is the structure of an order-3 Markov process---any 3 symbols will uniquely define a state (the converse happens to also be true in this instance)}{figure.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.2}Example of Slow Convergence : Even Process}{20}{subsection.1.7.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Excess entropy estimates for 50 instances of full order-3 Markov chains (see Fig.\nobreakspace  {}\ref  {fig:random_order3} for the topology). Notice that the estimates become extremely good (actually exact) at $L=3$. This is a consequence of the process being finite order Markov.}}{21}{figure.1.10}}
\newlabel{fig:E_random_order3}{{1.10}{21}{Excess entropy estimates for 50 instances of full order-3 Markov chains (see Fig.~\ref {fig:random_order3} for the topology). Notice that the estimates become extremely good (actually exact) at $L=3$. This is a consequence of the process being finite order Markov}{figure.1.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces The Even Process requires that all blocks of uninterrupted ones, with zeros on either side, be even in length.}}{21}{figure.1.11}}
\newlabel{fig:even_feM_gr}{{1.11}{21}{The Even Process requires that all blocks of uninterrupted ones, with zeros on either side, be even in length}{figure.1.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces The above table illustrates that for the Even Process, for any length $N$, there exists a word (all ones) such that prediction based on that word alone is different than prediction based on that word knowing that the previous symbol is 0. Notice that the optimal probabilities, those predicted after the block of ones is begun by a zero, are the same as those predicted by the appropriate induced causal state.}}{22}{table.1.1}}
\newlabel{tab:EvenProcessProbs}{{1.1}{22}{The above table illustrates that for the Even Process, for any length $N$, there exists a word (all ones) such that prediction based on that word alone is different than prediction based on that word knowing that the previous symbol is 0. Notice that the optimal probabilities, those predicted after the block of ones is begun by a zero, are the same as those predicted by the appropriate induced causal state}{table.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces The non-Markovianness of the Even Process leads to some members of the family having very slow convergence. (Left) Relative errors in the excess entropy estimates show that even considering correlation lengths up to 10 is grossly inadequate for a large collection of processes. (Right) Relative error of entropy rate estimates are very slow to approach zero for members on the blue end of the spectrum. This process serves as a key motivating example in the search for analytic forms for {\bf  E}.}}{23}{figure.1.12}}
\newlabel{fig:E_hmu_Even_all_reldiff}{{1.12}{23}{The non-Markovianness of the Even Process leads to some members of the family having very slow convergence. (Left) Relative errors in the excess entropy estimates show that even considering correlation lengths up to 10 is grossly inadequate for a large collection of processes. (Right) Relative error of entropy rate estimates are very slow to approach zero for members on the blue end of the spectrum. This process serves as a key motivating example in the search for analytic forms for \EE }{figure.1.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces Excess entropy estimates for the Even Process without access to the actual limit {\bf  E}. Its estimates increase in a very slow manner making claims about convergence, except for very trivial ones, difficult.}}{24}{figure.1.13}}
\newlabel{fig:E_Even_all}{{1.13}{24}{Excess entropy estimates for the Even Process without access to the actual limit \EE . Its estimates increase in a very slow manner making claims about convergence, except for very trivial ones, difficult}{figure.1.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Crypticity and Cryptic Order}{24}{section.1.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.1}Crypticity}{24}{subsection.1.8.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.2}Cryptic Order}{25}{subsection.1.8.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces This highlights the crypticity $\chi $ in orange as the difference between the state information $C_\mu $ and the predictive information ${\bf  E}$. In this sense, crypticity can be thought of as `modeling overhead'.}}{26}{figure.1.14}}
\newlabel{fig:3var_PastFutureS}{{1.14}{26}{This highlights the crypticity $\PC $ in orange as the difference between the state information $\Cmu $ and the predictive information $\EE $. In this sense, crypticity can be thought of as `modeling overhead'}{figure.1.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces An illustration of a process which is order-4 Markov. The past $H[\Past ]$ is shown as being stratified in the standard way. We can see that conditioning on the past 4 variables reduces as much uncertainty in the future as does conditioning on the entire past. Conditioning on only the past 3 variables, however, neglects the upper tip of the mutual information, ${\bf  E}=I[\Past ;\Future ]$.}}{27}{figure.1.15}}
\newlabel{fig:FoliationMarkov}{{1.15}{27}{An illustration of a process which is order-4 Markov. The past $H[\protect \Past ]$ is shown as being stratified in the standard way. We can see that conditioning on the past 4 variables reduces as much uncertainty in the future as does conditioning on the entire past. Conditioning on only the past 3 variables, however, neglects the upper tip of the mutual information, $\EE =I[\protect \Past ;\protect \Future ]$}{figure.1.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces This is another illustration of an order-4 Markov process. The causal state has been added to the diagram and the boundaries made a little more curvy to anticipate future I-diagrams. Notice that in addition to the length 4 statistics being sufficient for capturing ${\bf  E}$, the same is true for capturing $\chi $ which is the remainder of $C_\mu $. In contrast, the length 3 statistics are insufficient for both ${\bf  E}$ and $\chi $. Being insufficient for ${\bf  E}$ is why the process is order-4 Markov. Being insufficient for $\chi $ is why the process is order-4 cryptic.}}{27}{figure.1.16}}
\newlabel{fig:FoliationCrypticReqk}{{1.16}{27}{This is another illustration of an order-4 Markov process. The causal state has been added to the diagram and the boundaries made a little more curvy to anticipate future I-diagrams. Notice that in addition to the length 4 statistics being sufficient for capturing $\EE $, the same is true for capturing $\PC $ which is the remainder of $\Cmu $. In contrast, the length 3 statistics are insufficient for both $\EE $ and $\PC $. Being insufficient for $\EE $ is why the process is order-4 Markov. Being insufficient for $\PC $ is why the process is order-4 cryptic}{figure.1.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces An illustration of a process with differing cryptic and Markov orders. The Markov order is 4; this is the first history length which contains all of the predictive information. Notice that the length 3 history curves back again missing a portion of ${\bf  E}$. The cryptic order is 3 because although the length 3 history misses some portion of ${\bf  E}$, it does determine the causal state conditioned on the future. Note that $H[ {X} _{-3}^3]$ is labeled twice for clarity.}}{28}{figure.1.17}}
\newlabel{fig:FoliationCryptic}{{1.17}{28}{An illustration of a process with differing cryptic and Markov orders. The Markov order is 4; this is the first history length which contains all of the predictive information. Notice that the length 3 history curves back again missing a portion of $\EE $. The cryptic order is 3 because although the length 3 history misses some portion of $\EE $, it does determine the causal state conditioned on the future. Note that $H[\MeasSymbol _{-3}^3]$ is labeled twice for clarity}{figure.1.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.18}{\ignorespaces The Markov and cryptic orders may differ by more than one. This is an instance where the Markov order is 4, yet the cryptic order is 2. Two entropies are labeled twice for clarity.}}{28}{figure.1.18}}
\newlabel{fig:FoliationCrypticRgegek}{{1.18}{28}{The Markov and cryptic orders may differ by more than one. This is an instance where the Markov order is 4, yet the cryptic order is 2. Two entropies are labeled twice for clarity}{figure.1.18}{}}
\citation{Crut88a}
\citation{Crut98d}
\citation{Crut88a}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Prediction, Retrodiction and the Amount of Information Stored in the Present}{29}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:PRATISP}{{2}{29}{Prediction, Retrodiction and the Amount of Information Stored in the Present\relax }{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{29}{section.2.1}}
\citation{Crut08a}
\citation{Crut08c}
\citation{Crut08d}
\citation{Crut91b}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Retrodiction}{30}{section.2.2}}
\citation{Crut98d}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces  Hidden Process Lattice: The $ {X} $ variables denote the observed process; the $ \mathcal  {S} $ variables, the hidden states. If one scans the observed variables in the positive direction---seeing $ {X} _{-3}$, $ {X} _{-2}$, and $ {X} _{-1}$---then that history takes one to causal state $ { \mathcal  {S} }^{+} _0$. Analogously, if one scans in the reverse direction, then the succession of variables $ {X} _{2}$, $ {X} _{1}$, and $ {X} _{0}$ leads to $ { \mathcal  {S} }^{-} _0$. }}{31}{table.2.1}}
\newlabel{tab:ProcessLattice}{{2.1}{31}{ Hidden Process Lattice: The $\MeasSymbol $ variables denote the observed process; the $\CausalState $ variables, the hidden states. If one scans the observed variables in the positive direction---seeing $\MeasSymbol _{-3}$, $\MeasSymbol _{-2}$, and $\MeasSymbol _{-1}$---then that history takes one to causal state $\FutureCausalState _0$. Analogously, if one scans in the reverse direction, then the succession of variables $\MeasSymbol _{2}$, $\MeasSymbol _{1}$, and $\MeasSymbol _{0}$ leads to $\PastCausalState _0$. \relax }{table.2.1}{}}
\newlabel{prop:FuturePastEqPredict}{{1}{31}{Retrodiction\relax }{Prop.1}{}}
\citation{Crut91b}
\citation{Cove06a}
\citation{Crut08a}
\citation{Crut97a}
\citation{Feld98b}
\citation{Shal98a}
\newlabel{ProcessNotTimeSymmetric}{{2}{32}{Retrodiction\relax }{Prop.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Excess Entropy from Causal States}{32}{section.2.3}}
\newlabel{SpinEEandCmu}{{2.2}{32}{Excess Entropy from Causal States\relax }{equation.2.3.2}{}}
\citation{Crut08a}
\citation{Crut08c}
\citation{Yeun91a}
\citation{Crut08c}
\citation{Crut08a}
\newlabel{CmuEBound}{{2.3}{33}{Excess Entropy from Causal States\relax }{equation.2.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The generic (un-reduced) I-diagram for 4 random variables, where the names of the variables of interest have been inserted.}}{34}{figure.2.1}}
\newlabel{fig:4variable}{{2.1}{34}{The generic (un-reduced) I-diagram for 4 random variables, where the names of the variables of interest have been inserted}{figure.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The I-diagram for the forward and reverse \unhbox \voidb@x \hbox {$\epsilon $-machines}. Only 5 of the 15 independent information quantities remain. This image is a central reference for the work following.}}{35}{figure.2.2}}
\newlabel{fig:4var_eM}{{2.2}{35}{The I-diagram for the forward and reverse \eMs . Only 5 of the 15 independent information quantities remain. This image is a central reference for the work following}{figure.2.2}{}}
\newlabel{EasCausalMIEq}{{2.4}{35}{Excess Entropy from Causal States\relax }{equation.2.3.4}{}}
\newlabel{EasCausalMI}{{1}{35}{Excess Entropy from Causal States\relax }{equation.2.3.4}{}}
\newlabel{FutureCmuReln}{{2.7}{36}{Excess Entropy from Causal States\relax }{equation.2.3.7}{}}
\newlabel{PastCmuReln}{{2.8}{36}{Excess Entropy from Causal States\relax }{equation.2.3.8}{}}
\citation{Crut98d}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}The Bidirectional Machine}{37}{section.2.4}}
\citation{Crut01a}
\newlabel{SwitchingMapsAreOnto}{{4}{38}{The Bidirectional Machine\relax }{Prop.4}{}}
\citation{Crut98d}
\citation{Shal98a}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Upper Bounds}{39}{subsection.2.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Causal Irreversibility}{39}{subsection.2.4.2}}
\citation{Crut91b}
\newlabel{MR_SameEM}{{7}{40}{Causal Irreversibility\relax }{Prop.7}{}}
\newlabel{CI_MicroIrreversible}{{8}{40}{Causal Irreversibility\relax }{Prop.8}{}}
\citation{Shan49a}
\citation{Pack80}
\citation{Cove06a}
\citation{Crut87f}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Process Crypticity}{41}{subsection.2.4.3}}
\newlabel{Prop:PCisaDistance}{{9}{41}{Process Crypticity\relax }{Prop.9}{}}
\citation{Crut08a}
\citation{Crut08c}
\citation{Crut08d}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Alternative Presentations}{42}{section.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces This diagram summarizes the measures and relationships derived in this chapter. The upper part of the figure should already be familiar---some relationships have been added. The bottom three icons illustrate which portions of the above diagram are added (or subtracted) to obtain the three newly defined measures: $ C_\mu ^{\pm } $, $\Xi $, and $\chi ^{\pm }$. These represent the process's bidirectional information storage, irreversibility, and information overhead, respectively.}}{43}{figure.2.3}}
\newlabel{4var_eM_more3}{{2.3}{43}{This diagram summarizes the measures and relationships derived in this chapter. The upper part of the figure should already be familiar---some relationships have been added. The bottom three icons illustrate which portions of the above diagram are added (or subtracted) to obtain the three newly defined measures: $\BiCmu $, $\CI $, and $\BiPC $. These represent the process's bidirectional information storage, irreversibility, and information overhead, respectively}{figure.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Time-Reversed Presentation}{43}{subsection.2.5.1}}
\citation{Uppe97a}
\newlabel{eqpi}{{10}{44}{Time-Reversed Presentation\relax }{Prop.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Mixed-State Presentation}{44}{subsection.2.5.2}}
\newlabel{MSP}{{2.5.2}{44}{Mixed-State Presentation\relax }{subsection.2.5.2}{}}
\newlabel{mixedstates}{{2.16}{45}{Mixed-State Presentation\relax }{equation.2.5.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Calculating Excess Entropy}{45}{section.2.6}}
\newlabel{mixedstatesrev}{{2.22}{46}{Calculating Excess Entropy\relax }{equation.2.6.22}{}}
\newlabel{transitionrev}{{2.26}{46}{Calculating Excess Entropy\relax }{equation.2.6.26}{}}
\citation{Cove06a}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Calculational Example}{47}{section.2.7}}
\citation{Uppe97a}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  The presentations used to calculate the excess entropy for the RnC Process: (a)\nobreakspace  {}$ { M }^{+} $, (b)\nobreakspace  {}\unhbox \voidb@x \hbox {$\mathaccentV {widetilde}365{M}^+ = \mathcal  {T}( { M }^{+} )$}, and (c)\nobreakspace  {}\unhbox \voidb@x \hbox {$ { M }^{-} = \mathcal  {U}(\mathaccentV {widetilde}365{M}^+)$}. Edge labels $t|x$ give the probability \unhbox \voidb@x \hbox {$t = T_{ \mathcal  {R}  \mathcal  {R} ^\prime }^{(x)}$} of making a transition and seeing symbol $x$. }}{49}{figure.2.4}}
\newlabel{fig:RnC}{{2.4}{49}{ The presentations used to calculate the excess entropy for the RnC Process: (a)~$\FutureEM $, (b)~\mbox {$\widetilde {M}^+ = \TR (\FutureEM )$}, and (c)~\mbox {$\PastEM = \MSP (\widetilde {M}^+)$}. Edge labels $t|x$ give the probability \mbox {$t = T_{\AlternateState \AlternateState ^\prime }^{(x)}$} of making a transition and seeing symbol $x$. \relax }{figure.2.4}{}}
\citation{Weis73}
\citation{Crut01a}
\citation{Crut01a}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Examples}{51}{section.2.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.1}Even Process}{51}{subsection.2.8.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  Forward and reverse \unhbox \voidb@x \hbox {$\epsilon $-machines}\ for the Even Process: (a) $ { M }^{+} $ and (b) $ { M }^{-} $. (c) The bidirectional machine $ { M }^{\pm } $. Edge labels are prefixed by the scan direction $\{-,+\}$. }}{52}{figure.2.5}}
\newlabel{fig:EvenProcess}{{2.5}{52}{ Forward and reverse \eMs \ for the Even Process: (a) $\FutureEM $ and (b) $\PastEM $. (c) The bidirectional machine $\BiEM $. Edge labels are prefixed by the scan direction $\{-,+\}$. \relax }{figure.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The Even Process's information processing properties---$ C_\mu ^{\pm } $, $ C_\mu ^{+} $, and $\chi ^{+}$---as its self-loop probability $p$ varies. The colored area bounded by the curves show the magnitude of ${\bf  E}$. }}{53}{figure.2.6}}
\newlabel{fig:EP_info}{{2.6}{53}{The Even Process's information processing properties---$\BiCmu $, $\FutureCmu $, and $\FuturePC $---as its self-loop probability $p$ varies. The colored area bounded by the curves show the magnitude of $\EE $. \relax }{figure.2.6}{}}
\citation{Crut01a}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.2}Golden Mean Process}{54}{subsection.2.8.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces  Forward and reverse \unhbox \voidb@x \hbox {$\epsilon $-machines}\ for the Golden Mean Process: (a) $ { M }^{+} $ and (b) $ { M }^{-} $. (c) The bidirectional machine $ { M }^{\pm } $. }}{55}{figure.2.7}}
\newlabel{fig:GMP}{{2.7}{55}{ Forward and reverse \eMs \ for the Golden Mean Process: (a) $\FutureEM $ and (b) $\PastEM $. (c) The bidirectional machine $\BiEM $. \relax }{figure.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces The Golden Mean Process's information processing measures---$ C_\mu ^{\pm } $, $ C_\mu ^{+} $, and $\chi ^{+}$---as its self-loop probability $p$ varies. Colored areas bounded by the curves give the magnitude at each $p$ of $\chi ^{-}$, ${\bf  E}$, and $\chi ^{+}$. }}{56}{figure.2.8}}
\newlabel{fig:GMP_info}{{2.8}{56}{The Golden Mean Process's information processing measures---$\BiCmu $, $\FutureCmu $, and $\FuturePC $---as its self-loop probability $p$ varies. Colored areas bounded by the curves give the magnitude at each $p$ of $\PastPC $, $\EE $, and $\FuturePC $. \relax }{figure.2.8}{}}
\citation{Crut08a}
\citation{Crut08a}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.3}Random Insertion Process}{58}{subsection.2.8.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces  Forward and reverse \unhbox \voidb@x \hbox {$\epsilon $-machines}\ for the RIP with $p=q=1/2$: (a) $ { M }^{+} $ and (b) $ { M }^{-} $. (c) The bidirectional machine $ { M }^{\pm } $ also for $p = q = 1/2$. (Reprinted with permission from Refs.\nobreakspace  {}\cite  {Crut08a}.) }}{59}{figure.2.9}}
\newlabel{fig:RIP}{{2.9}{59}{ Forward and reverse \eMs \ for the RIP with $p=q=1/2$: (a) $\FutureEM $ and (b) $\PastEM $. (c) The bidirectional machine $\BiEM $ also for $p = q = 1/2$. (Reprinted with permission from Refs.~\cite {Crut08a}.) \relax }{figure.2.9}{}}
\citation{Fras90b}
\citation{Casd91a}
\citation{Spro03a}
\citation{Kant06a}
\citation{Arno96}
\citation{Crut97a}
\citation{Feld98b}
\citation{Tono94a}
\citation{Bial00a}
\citation{Ebel94c}
\citation{Debo08a}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Conclusions}{60}{section.2.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces The Random Insertion Process's information processing measures as its two probability parameters $p$ and $q$ vary. The central square shows the $(p,q)$ parameter space, with solid and dashed lines indicating the paths in parameter space for each of the other information versus parameter plots. The latter's vertical axes are scaled so that two tick marks measure $1$ bit of information. The inset legend indicates the class of process illustrated by the paths. Colored areas give the magnitude of $\chi ^{-}$, ${\bf  E}$, and $\chi ^{+}$. }}{63}{figure.2.10}}
\newlabel{fig:RIP_info}{{2.10}{63}{The Random Insertion Process's information processing measures as its two probability parameters $p$ and $q$ vary. The central square shows the $(p,q)$ parameter space, with solid and dashed lines indicating the paths in parameter space for each of the other information versus parameter plots. The latter's vertical axes are scaled so that two tick marks measure $1$ bit of information. The inset legend indicates the class of process illustrated by the paths. Colored areas give the magnitude of $\PastPC $, $\EE $, and $\FuturePC $. \relax }{figure.2.10}{}}
\citation{Crut08a}
\citation{Crut08b}
\citation{Fras90b}
\citation{Casd91a}
\citation{Spro03a}
\citation{Kant06a}
\citation{Arno96}
\citation{Crut97a}
\citation{Feld98b}
\citation{Tono94a}
\citation{Bial00a}
\citation{Ebel94c}
\citation{Debo08a}
\citation{Crut08a}
\citation{Crut08b}
\citation{Crut08a}
\citation{Crut08b}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Information Accessibility and Cryptic Processes}{64}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:IACP}{{3}{64}{Information Accessibility and Cryptic Processes\relax }{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{64}{section.3.1}}
\citation{Crut08a}
\citation{Crut08b}
\newlabel{eq:EForRevState}{{3.1}{65}{Introduction\relax }{equation.3.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}k-Crypticity}{66}{section.3.2}}
\newlabel{Def:OrderKCrypticityCriterion}{{3.2}{66}{k-Crypticity\relax }{equation.3.2.2}{}}
\newlabel{lem:hSkgF}{{1}{66}{k-Crypticity\relax }{Lem.1}{}}
\newlabel{lem:OKCOJC}{{2}{66}{k-Crypticity\relax }{Lem.2}{}}
\citation{Crut08a}
\citation{Crut08b}
\newlabel{prop:MarkovOKC}{{11}{67}{k-Crypticity\relax }{Prop.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}The $k$-Cryptic Expansion}{67}{subsection.3.2.1}}
\newlabel{eq:CrypticityExpansion}{{3.3}{67}{The $k$-Cryptic Expansion\relax }{equation.3.2.3}{}}
\newlabel{thm:OKCC}{{3}{67}{The $k$-Cryptic Expansion\relax }{equation.3.2.3}{}}
\newlabel{cor:OKCC}{{6}{68}{The $k$-Cryptic Expansion\relax }{Cor.6}{}}
\newlabel{cor:ZeroCrypticity}{{7}{69}{The $k$-Cryptic Expansion\relax }{Cor.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Convergence}{69}{subsection.3.2.2}}
\newlabel{prop:NonDecreasingCrypticity}{{12}{69}{Convergence\relax }{Prop.12}{}}
\newlabel{cor:ReachChiStayChi}{{8}{69}{Convergence\relax }{Cor.8}{}}
\newlabel{cor:Chik0Chi10}{{9}{69}{Convergence\relax }{Cor.9}{}}
\newlabel{cor:Chi10Chik0}{{10}{69}{Convergence\relax }{Cor.10}{}}
\newlabel{cor:TrivialChi}{{11}{70}{Convergence\relax }{Cor.11}{}}
\newlabel{prop:ChiConverges}{{13}{70}{Convergence\relax }{Prop.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An I-diagram helps to organize the algebra. Note that we reduce the complexity of this diagram by making two of the variables aggregate variables. Also, we have opted for an alternate representation of the I-diagram keeping three of the regions circular.}}{72}{figure.3.1}}
\newlabel{fig:4var_snake}{{3.1}{72}{An I-diagram helps to organize the algebra. Note that we reduce the complexity of this diagram by making two of the variables aggregate variables. Also, we have opted for an alternate representation of the I-diagram keeping three of the regions circular}{figure.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Excess Entropy for $k$-Cryptic Processes}{72}{subsection.3.2.3}}
\citation{Crut08a}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The entropy growth functions: block entropy $H[ {X} _0^L]$, block-state entropy $H[ {X} _0^L,  \mathcal  {S} _L]$, and state-block entropy $H[ \mathcal  {S} _0,  {X} _0^L]$ provide a convenient way for understanding several of a process's properties. Previously, the entropy rate, excess entropy, and Markov order were seen on this diagram. We now add statistical complexity, crypticity, and cryptic order to that list. A pleasing feature of this figure is that it reproduces the I-diagram in Fig.\nobreakspace  {}\ref  {fig:FoliationCryptic} when viewed end on.}}{73}{figure.3.2}}
\newlabel{fig:EntropyGrowthCurvescryptic}{{3.2}{73}{The entropy growth functions: block entropy $H[\MeasSymbol _0^L]$, block-state entropy $H[\MeasSymbol _0^L, \CausalState _L]$, and state-block entropy $H[\CausalState _0, \MeasSymbol _0^L]$ provide a convenient way for understanding several of a process's properties. Previously, the entropy rate, excess entropy, and Markov order were seen on this diagram. We now add statistical complexity, crypticity, and cryptic order to that list. A pleasing feature of this figure is that it reproduces the I-diagram in Fig.~\ref {fig:FoliationCryptic} when viewed end on}{figure.3.2}{}}
\newlabel{cor:OKCE}{{12}{73}{Excess Entropy for $k$-Cryptic Processes\relax }{Cor.12}{}}
\newlabel{cor:EEEqualCmu}{{13}{73}{Excess Entropy for $k$-Cryptic Processes\relax }{Cor.13}{}}
\citation{Crut97a}
\citation{Crut97a}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Crypticity of Spin Chains}{74}{subsection.3.2.4}}
\newlabel{prop:SpinChainCondition}{{15}{74}{Crypticity of Spin Chains\relax }{equation.3.2.4}{}}
\citation{Crut01a}
\citation{Crut08a}
\citation{Crut08b}
\citation{Crut08a}
\newlabel{prop:SpinChainNotRMinus1}{{17}{75}{Crypticity of Spin Chains\relax }{Prop.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Examples}{75}{section.3.3}}
\citation{Crut08b}
\citation{Crut08b}
\citation{Crut01a}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Even Process: $0$-Cryptic}{76}{subsection.3.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A $0$\nobreakdash -cryptic process: Even Process. The transitions denote the probability $p$ of generating symbol $x$ as $p|x$.}}{76}{figure.3.3}}
\newlabel{fig:EvenProcess}{{3.3}{76}{A \cryptic {0} process: Even Process. The transitions denote the probability $p$ of generating symbol $x$ as $p|x$}{figure.3.3}{}}
\citation{Crut01a}
\citation{Crut08b}
\citation{Crut01a}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Golden Mean Process: $1$-Cryptic}{77}{subsection.3.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A $1$\nobreakdash -cryptic process: Golden Mean Process. }}{77}{figure.3.4}}
\newlabel{fig:GoldenMean}{{3.4}{77}{A \cryptic {1} process: Golden Mean Process. \relax }{figure.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Butterfly Process: $2$-Cryptic}{78}{subsection.3.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces A $2$\nobreakdash -cryptic process: Butterfly Process over a $6$-symbol alphabet.}}{79}{figure.3.5}}
\newlabel{fig:ButterflyProcess}{{3.5}{79}{A \cryptic {2} process: Butterfly Process over a $6$-symbol alphabet}{figure.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Restricted Golden Mean: $k$-Cryptic}{80}{subsection.3.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces $k$\nobreakdash -cryptic processes: Restricted Golden Mean Family.}}{80}{figure.3.6}}
\newlabel{fig:RestrictedGM}{{3.6}{80}{\cryptic {k} processes: Restricted Golden Mean Family}{figure.3.6}{}}
\citation{Maho09b}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Stretched Golden Mean}{81}{subsection.3.3.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces $k$\nobreakdash -cryptic processes: Stretched Golden Mean Family.}}{82}{figure.3.7}}
\newlabel{fig:StretchedGM}{{3.7}{82}{\cryptic {k} processes: Stretched Golden Mean Family}{figure.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.6}Nemo Process: $\infty $-Cryptic}{82}{subsection.3.3.6}}
\citation{Crut98d}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces The $\infty $\nobreakdash -cryptic Nemo Process. }}{83}{figure.3.8}}
\newlabel{fig:Nemo}{{3.8}{83}{The \cryptic {\infty } Nemo Process. \relax }{figure.3.8}{}}
\citation{Crut08a}
\citation{Crut08b}
\citation{Maho09b}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces This figure shows a bird's-eye view of process space. Some sample processes were chosen and placed on a plot of Markov vs cryptic order. Some \unhbox \voidb@x \hbox {$\epsilon $-machines}\ point to particukar points in the space while others are parameterized \unhbox \voidb@x \hbox {$\epsilon $-machines}\ and refer to a colored region. We can readily see that aside from the bound $R\ge k$, the space is filled. This means that the cryptic order is a nontrivial complement to Markov order.}}{86}{figure.3.9}}
\newlabel{}{{3.9}{86}{This figure shows a bird's-eye view of process space. Some sample processes were chosen and placed on a plot of Markov vs cryptic order. Some \eMs \ point to particukar points in the space while others are parameterized \eMs \ and refer to a colored region. We can readily see that aside from the bound $R\ge k$, the space is filled. This means that the cryptic order is a nontrivial complement to Markov order}{figure.3.9}{}}
\citation{Crut01a}
\citation{Crut91e}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Conclusion}{87}{section.3.4}}
\citation{Crut08a}
\citation{Crut08b}
\citation{Crut08a}
\citation{Crut08b}
\citation{Maho09a}
\citation{Crut08a}
\citation{Crut08b}
\citation{Maho09a}
\citation{Maho09a}
\citation{Maho09a}
\citation{Crut08a}
\citation{Crut08b}
\citation{Maho09a}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Information Accessibility and Cryptic Processes: Linear Combinations of Causal States}{89}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:IACPLOCS}{{4}{89}{Information Accessibility and Cryptic Processes: Linear Combinations of Causal States\relax }{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{89}{section.4.1}}
\citation{Maho09a}
\citation{Crut08a}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A $2$\nobreakdash -cryptic process: The \unhbox \voidb@x \hbox {$\epsilon $-machine}\ representation of the Butterfly Process. Edge labels $t| {x} $ give the probability $t = T^{( {x} )}_{ \sigma  {\sigma ^{\prime }} }$ of making a transition and from causal state $ \sigma $ to causal state $ {\sigma ^{\prime }} $ and seeing symbol $ {x} $. }}{90}{figure.4.1}}
\newlabel{fig:ButterflyProcess}{{4.1}{90}{A \cryptic {2} process: The \eM \ representation of the Butterfly Process. Edge labels $t|\meassymbol $ give the probability $t = T^{(\meassymbol )}_{\causalstate \causalstateprime }$ of making a transition and from causal state $\causalstate $ to causal state $\causalstateprime $ and seeing symbol $\meassymbol $. \relax }{figure.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Butterfly Process}{90}{section.4.2}}
\citation{Maho09a}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Time-reversed Butterfly Process.}}{91}{figure.4.2}}
\newlabel{fig:TRButterflyProcess}{{4.2}{91}{Time-reversed Butterfly Process}{figure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Reverse Butterfly Process. }}{92}{figure.4.3}}
\newlabel{fig:ReverseButterflyProcess}{{4.3}{92}{Reverse Butterfly Process. \relax }{figure.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Restricted Golden Mean Process}{92}{section.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The \unhbox \voidb@x \hbox {$\epsilon $-machine}\ for the Restricted Golden Mean Process.}}{93}{figure.4.4}}
\newlabel{fig:RestrictedGM}{{4.4}{93}{The \eM \ for the Restricted Golden Mean Process}{figure.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Time-reversed presentation of the Restricted Golden Mean Process.}}{94}{figure.4.5}}
\newlabel{fig:TRRestrictedGM}{{4.5}{94}{Time-reversed presentation of the Restricted Golden Mean Process}{figure.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Reverse Restricted Golden Mean Process.}}{95}{figure.4.6}}
\newlabel{fig:ReverseRestrictedGM}{{4.6}{95}{Reverse Restricted Golden Mean Process}{figure.4.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The \unhbox \voidb@x \hbox {$\epsilon $-machine}\ for the $\infty $-cryptic Nemo Process.}}{95}{figure.4.7}}
\newlabel{fig:Nemo}{{4.7}{95}{The \eM \ for the $\infty $-cryptic Nemo Process}{figure.4.7}{}}
\citation{Maho09a}
\citation{Maho09a}
\citation{Crut08a}
\citation{Crut08b}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Nemo Process}{96}{section.4.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces The time-reversed presentation, $\mathaccentV {widetilde}365{M}^+ = \mathcal  {T}( { M }^{+} )$, of the Nemo Process. }}{97}{figure.4.8}}
\newlabel{fig:TRNemo}{{4.8}{97}{The time-reversed presentation, $\widetilde {M}^+ = \TR (\FutureEM )$, of the Nemo Process. \relax }{figure.4.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces The reverse \unhbox \voidb@x \hbox {$\epsilon $-machine}\ for the Nemo Process. }}{98}{figure.4.9}}
\newlabel{fig:ReverseNemo}{{4.9}{98}{The reverse \eM \ for the Nemo Process. \relax }{figure.4.9}{}}
\citation{Crut08a}
\citation{Crut08b}
\citation{Maho09a}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Conclusion}{99}{section.4.5}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Calculating the time-reversed Butterfly Process's \unhbox \voidb@x \hbox {$\epsilon $-machine}\ via the forward \unhbox \voidb@x \hbox {$\epsilon $-machine}'s mixed states. The $5$-vector denotes the mixed-state distribution $\mu (w)$ reached after having seen the corresponding allowed word $w$. If the word leads to a unique state with probability one, we give instead the state's name. }}{100}{table.4.1}}
\newlabel{tab:ButterflyProcessMixedStates}{{4.1}{100}{Calculating the time-reversed Butterfly Process's \eM \ via the forward \eM 's mixed states. The $5$-vector denotes the mixed-state distribution $\mu (w)$ reached after having seen the corresponding allowed word $w$. If the word leads to a unique state with probability one, we give instead the state's name. \relax }{table.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Calculating the reversed RGMP using mixed states over the \unhbox \voidb@x \hbox {$\epsilon $-machine}\ states. }}{101}{table.4.2}}
\newlabel{tab:RestrictedGMMixedStates}{{4.2}{101}{Calculating the reversed RGMP using mixed states over the \eM \ states. \relax }{table.4.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Commentary on information measures}{102}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:commentary}{{A}{102}{Commentary on information measures\relax }{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Entropy rate for bicyclists}{102}{section.A.1}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Statistical Complexity for ...}{103}{section.A.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Venn Diagrams and Information Theory}{104}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:Venn}{{B}{104}{Venn Diagrams and Information Theory\relax }{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Venn Diagrams and Information Theory}{104}{section.B.1}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}How to read an I-diagram}{105}{section.B.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces The simplest I-diagram - one random variable, $X$.}}{105}{figure.B.1}}
\newlabel{fig:1var_bare}{{B.1}{105}{The simplest I-diagram - one random variable, $X$}{figure.B.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces I-diagram for two random variables, $X$ and $Y$.}}{105}{figure.B.2}}
\newlabel{fig:2var_bare}{{B.2}{105}{I-diagram for two random variables, $X$ and $Y$}{figure.B.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces The mutual information between $X$ and $Y$ is highlighted.}}{106}{figure.B.3}}
\newlabel{fig:2var_IXY}{{B.3}{106}{The mutual information between $X$ and $Y$ is highlighted}{figure.B.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces The conditional entropy of $X$ given $Y$ is highlighted.}}{106}{figure.B.4}}
\newlabel{fig:2var_HX_Y}{{B.4}{106}{The conditional entropy of $X$ given $Y$ is highlighted}{figure.B.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.1}Stratification of a Composite Variable}{106}{subsection.B.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.5}{\ignorespaces The conditional entropy of $X$ given $Y$ and $Z$ is highlighted.}}{107}{figure.B.5}}
\newlabel{fig:3var_HX_YZ}{{B.5}{107}{The conditional entropy of $X$ given $Y$ and $Z$ is highlighted}{figure.B.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.6}{\ignorespaces The mutual information of $X$ and joint variable $W$ is highlighted.}}{108}{figure.B.6}}
\newlabel{fig:3var_HX_W}{{B.6}{108}{The mutual information of $X$ and joint variable $W$ is highlighted}{figure.B.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.7}{\ignorespaces The conditional mutual entropy of $X$ and $Y$ given $Z$ is highlighted.}}{108}{figure.B.7}}
\newlabel{fig:3var_IXY_Z}{{B.7}{108}{The conditional mutual entropy of $X$ and $Y$ given $Z$ is highlighted}{figure.B.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.8}{\ignorespaces The standard stratification of the conglomerate random variable $\Past $.}}{109}{figure.B.8}}
\newlabel{fig:FoliationPast}{{B.8}{109}{The standard stratification of the conglomerate random variable $\protect \Past $}{figure.B.8}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Proofs}{110}{appendix.C}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:proofs}{{C}{110}{Proofs\relax }{appendix.C}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C.1}Entropic Independence $\Rightarrow $ Probabilistic Independence}{110}{section.C.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {D}Mixed-State Presentation is Sufficient to Calculate the Switching Maps}{112}{appendix.D}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:EFromMixedState}{{D}{112}{Mixed-State Presentation is Sufficient to Calculate the Switching Maps\relax }{appendix.D}{}}
\bibstyle{../bibliography/expanded}
\bibdata{../bibliography/references}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{114}{section*.8}}
