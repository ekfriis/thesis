\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces IID binary process.}}{8}{figure.1.1}
\contentsline {figure}{\numberline {1.2}{\ignorespaces (Left) Block entropy curve for all binary IID processes ($\qopname \relax m{Pr}( {X} =0) = p$). Each is simply a line through the origin with slope determined by $p$. Since there is a symmetry in the class about $p=0.5$, only on half of the $p$ values are illustrated. (Right) Finite length entropy rate approximations. Colorbar indicates the probability, $p$, that is varied to obtain different members of a process family. The particular probability, $p$, refers to the variable in Fig.\nobreakspace {}\ref {fig:IID_eM}.}}{10}{figure.1.2}
\contentsline {figure}{\numberline {1.3}{\ignorespaces The Golden Mean Process is one that disallows consecutive zeros. After a zero [one] is seen, the process is in causal state $B$ $[A]$.}}{11}{figure.1.3}
\contentsline {figure}{\numberline {1.4}{\ignorespaces (Left) Block entropy curves for all Golden Mean processes. Each is linear for $L\ge 1$. (Right) Each finite length entropy rate estimate reaches its asymptotic value $h_\mu $, at $L=2$. This indicates that the additional uncertainty in the $L=2$ blocks, beyond the $L=1$ blocks, is already $h_\mu $. This implies that the minimum correlation length required for maximal prediction ability is $L=1$. That is, the Golden Mean is an order\nobreakdash -$1$ Markov process.}}{12}{figure.1.4}
\contentsline {figure}{\numberline {1.5}{\ignorespaces Nonunifilar presentation of the Golden Mean Process. The entropy rate of the process is \emph {not} simply the weighted average of the entropies of symbols emitted after visiting each state.}}{13}{figure.1.5}
\contentsline {figure}{\numberline {1.6}{\ignorespaces A stochastic process can be viewed as a communication channel. The data in the past is the input to the channel. The channel itself is the dynamical system, or \unhbox \voidb@x \hbox {$\epsilon $-machine}, which transmits information to the future. The total information transmitted from past to future is equal to the excess entropy.}}{17}{figure.1.6}
\contentsline {figure}{\numberline {1.7}{\ignorespaces This I-diagram highlights the role of excess entropy as the mutual information between past and future data.}}{17}{figure.1.7}
\contentsline {figure}{\numberline {1.8}{\ignorespaces The generic relation among the past, future and a state, $ \mathcal {R} $ includes 15 nontrivial information quantities. Demanding that the state involved is a causal state effects 4 of these quantities. The green area (which corresponds to two information atoms) is zero because the causal state is a single-valued function of the past. The purple area is zero because causal states are prescient. The orange area is not zero, but is the minimum value possible, given that green and purple are zero.}}{18}{figure.1.8}
\contentsline {figure}{\numberline {1.9}{\ignorespaces A process with 8 causal states. Since each state has two outgoing transitions, each of which has one free parameter, we suppress the probabilities here. The reader may verify that this is the structure of an order-3 Markov process---any 3 symbols will uniquely define a state (the converse happens to also be true in this instance).}}{20}{figure.1.9}
\contentsline {figure}{\numberline {1.10}{\ignorespaces Excess entropy estimates for 50 instances of full order-3 Markov chains (see Fig.\nobreakspace {}\ref {fig:random_order3} for the topology). Notice that the estimates become extremely good (actually exact) at $L=3$. This is a consequence of the process being finite order Markov.}}{21}{figure.1.10}
\contentsline {figure}{\numberline {1.11}{\ignorespaces The Even Process requires that all blocks of uninterrupted ones, with zeros on either side, be even in length.}}{21}{figure.1.11}
\contentsline {figure}{\numberline {1.12}{\ignorespaces The non-Markovianness of the Even Process leads to some members of the family having very slow convergence. (Left) Relative errors in the excess entropy estimates show that even considering correlation lengths up to 10 is grossly inadequate for a large collection of processes. (Right) Relative error of entropy rate estimates are very slow to approach zero for members on the blue end of the spectrum. This process serves as a key motivating example in the search for analytic forms for {\bf E}.}}{23}{figure.1.12}
\contentsline {figure}{\numberline {1.13}{\ignorespaces Excess entropy estimates for the Even Process without access to the actual limit {\bf E}. Its estimates increase in a very slow manner making claims about convergence, except for very trivial ones, difficult.}}{24}{figure.1.13}
\contentsline {figure}{\numberline {1.14}{\ignorespaces This highlights the crypticity $\chi $ in orange as the difference between the state information $C_\mu $ and the predictive information ${\bf E}$. In this sense, crypticity can be thought of as `modeling overhead'.}}{26}{figure.1.14}
\contentsline {figure}{\numberline {1.15}{\ignorespaces An illustration of a process which is order-4 Markov. The past $H[\Past ]$ is shown as being stratified in the standard way. We can see that conditioning on the past 4 variables reduces as much uncertainty in the future as does conditioning on the entire past. Conditioning on only the past 3 variables, however, neglects the upper tip of the mutual information, ${\bf E}=I[\Past ;\Future ]$.}}{27}{figure.1.15}
\contentsline {figure}{\numberline {1.16}{\ignorespaces This is another illustration of an order-4 Markov process. The causal state has been added to the diagram and the boundaries made a little more curvy to anticipate future I-diagrams. Notice that in addition to the length 4 statistics being sufficient for capturing ${\bf E}$, the same is true for capturing $\chi $ which is the remainder of $C_\mu $. In contrast, the length 3 statistics are insufficient for both ${\bf E}$ and $\chi $. Being insufficient for ${\bf E}$ is why the process is order-4 Markov. Being insufficient for $\chi $ is why the process is order-4 cryptic.}}{27}{figure.1.16}
\contentsline {figure}{\numberline {1.17}{\ignorespaces An illustration of a process with differing cryptic and Markov orders. The Markov order is 4; this is the first history length which contains all of the predictive information. Notice that the length 3 history curves back again missing a portion of ${\bf E}$. The cryptic order is 3 because although the length 3 history misses some portion of ${\bf E}$, it does determine the causal state conditioned on the future. Note that $H[ {X} _{-3}^3]$ is labeled twice for clarity.}}{28}{figure.1.17}
\contentsline {figure}{\numberline {1.18}{\ignorespaces The Markov and cryptic orders may differ by more than one. This is an instance where the Markov order is 4, yet the cryptic order is 2. Two entropies are labeled twice for clarity.}}{28}{figure.1.18}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The generic (un-reduced) I-diagram for 4 random variables, where the names of the variables of interest have been inserted.}}{34}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces The I-diagram for the forward and reverse \unhbox \voidb@x \hbox {$\epsilon $-machines}. Only 5 of the 15 independent information quantities remain. This image is a central reference for the work following.}}{35}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces This diagram summarizes the measures and relationships derived in this chapter. The upper part of the figure should already be familiar---some relationships have been added. The bottom three icons illustrate which portions of the above diagram are added (or subtracted) to obtain the three newly defined measures: $ C_\mu ^{\pm } $, $\Xi $, and $\chi ^{\pm }$. These represent the process's bidirectional information storage, irreversibility, and information overhead, respectively.}}{43}{figure.2.3}
\contentsline {figure}{\numberline {2.4}{\ignorespaces The presentations used to calculate the excess entropy for the RnC Process: (a)\nobreakspace {}$ { M }^{+} $, (b)\nobreakspace {}\unhbox \voidb@x \hbox {$\mathaccentV {widetilde}365{M}^+ = \mathcal {T}( { M }^{+} )$}, and (c)\nobreakspace {}\unhbox \voidb@x \hbox {$ { M }^{-} = \mathcal {U}(\mathaccentV {widetilde}365{M}^+)$}. Edge labels $t|x$ give the probability \unhbox \voidb@x \hbox {$t = T_{ \mathcal {R} \mathcal {R} ^\prime }^{(x)}$} of making a transition and seeing symbol $x$. }}{49}{figure.2.4}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Forward and reverse \unhbox \voidb@x \hbox {$\epsilon $-machines}\ for the Even Process: (a) $ { M }^{+} $ and (b) $ { M }^{-} $. (c) The bidirectional machine $ { M }^{\pm } $. Edge labels are prefixed by the scan direction $\{-,+\}$. }}{52}{figure.2.5}
\contentsline {figure}{\numberline {2.6}{\ignorespaces The Even Process's information processing properties---$ C_\mu ^{\pm } $, $ C_\mu ^{+} $, and $\chi ^{+}$---as its self-loop probability $p$ varies. The colored area bounded by the curves show the magnitude of ${\bf E}$. }}{53}{figure.2.6}
\contentsline {figure}{\numberline {2.7}{\ignorespaces Forward and reverse \unhbox \voidb@x \hbox {$\epsilon $-machines}\ for the Golden Mean Process: (a) $ { M }^{+} $ and (b) $ { M }^{-} $. (c) The bidirectional machine $ { M }^{\pm } $. }}{55}{figure.2.7}
\contentsline {figure}{\numberline {2.8}{\ignorespaces The Golden Mean Process's information processing measures---$ C_\mu ^{\pm } $, $ C_\mu ^{+} $, and $\chi ^{+}$---as its self-loop probability $p$ varies. Colored areas bounded by the curves give the magnitude at each $p$ of $\chi ^{-}$, ${\bf E}$, and $\chi ^{+}$. }}{56}{figure.2.8}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Forward and reverse \unhbox \voidb@x \hbox {$\epsilon $-machines}\ for the RIP with $p=q=1/2$: (a) $ { M }^{+} $ and (b) $ { M }^{-} $. (c) The bidirectional machine $ { M }^{\pm } $ also for $p = q = 1/2$. (Reprinted with permission from Refs.\nobreakspace {}\cite {Crut08a}.) }}{59}{figure.2.9}
\contentsline {figure}{\numberline {2.10}{\ignorespaces The Random Insertion Process's information processing measures as its two probability parameters $p$ and $q$ vary. The central square shows the $(p,q)$ parameter space, with solid and dashed lines indicating the paths in parameter space for each of the other information versus parameter plots. The latter's vertical axes are scaled so that two tick marks measure $1$ bit of information. The inset legend indicates the class of process illustrated by the paths. Colored areas give the magnitude of $\chi ^{-}$, ${\bf E}$, and $\chi ^{+}$. }}{63}{figure.2.10}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces An I-diagram helps to organize the algebra. Note that we reduce the complexity of this diagram by making two of the variables aggregate variables. Also, we have opted for an alternate representation of the I-diagram keeping three of the regions circular.}}{72}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces The entropy growth functions: block entropy $H[ {X} _0^L]$, block-state entropy $H[ {X} _0^L, \mathcal {S} _L]$, and state-block entropy $H[ \mathcal {S} _0, {X} _0^L]$ provide a convenient way for understanding several of a process's properties. Previously, the entropy rate, excess entropy, and Markov order were seen on this diagram. We now add statistical complexity, crypticity, and cryptic order to that list. A pleasing feature of this figure is that it reproduces the I-diagram in Fig.\nobreakspace {}\ref {fig:FoliationCryptic} when viewed end on.}}{73}{figure.3.2}
\contentsline {figure}{\numberline {3.3}{\ignorespaces A $0$\nobreakdash -cryptic process: Even Process. The transitions denote the probability $p$ of generating symbol $x$ as $p|x$.}}{76}{figure.3.3}
\contentsline {figure}{\numberline {3.4}{\ignorespaces A $1$\nobreakdash -cryptic process: Golden Mean Process. }}{77}{figure.3.4}
\contentsline {figure}{\numberline {3.5}{\ignorespaces A $2$\nobreakdash -cryptic process: Butterfly Process over a $6$-symbol alphabet.}}{79}{figure.3.5}
\contentsline {figure}{\numberline {3.6}{\ignorespaces $k$\nobreakdash -cryptic processes: Restricted Golden Mean Family.}}{80}{figure.3.6}
\contentsline {figure}{\numberline {3.7}{\ignorespaces $k$\nobreakdash -cryptic processes: Stretched Golden Mean Family.}}{82}{figure.3.7}
\contentsline {figure}{\numberline {3.8}{\ignorespaces The $\infty $\nobreakdash -cryptic Nemo Process. }}{83}{figure.3.8}
\contentsline {figure}{\numberline {3.9}{\ignorespaces This figure shows a bird's-eye view of process space. Some sample processes were chosen and placed on a plot of Markov vs cryptic order. Some \unhbox \voidb@x \hbox {$\epsilon $-machines}\ point to particukar points in the space while others are parameterized \unhbox \voidb@x \hbox {$\epsilon $-machines}\ and refer to a colored region. We can readily see that aside from the bound $R\ge k$, the space is filled. This means that the cryptic order is a nontrivial complement to Markov order.}}{86}{figure.3.9}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces A $2$\nobreakdash -cryptic process: The \unhbox \voidb@x \hbox {$\epsilon $-machine}\ representation of the Butterfly Process. Edge labels $t| {x} $ give the probability $t = T^{( {x} )}_{ \sigma {\sigma ^{\prime }} }$ of making a transition and from causal state $ \sigma $ to causal state $ {\sigma ^{\prime }} $ and seeing symbol $ {x} $. }}{90}{figure.4.1}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Time-reversed Butterfly Process.}}{91}{figure.4.2}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Reverse Butterfly Process. }}{92}{figure.4.3}
\contentsline {figure}{\numberline {4.4}{\ignorespaces The \unhbox \voidb@x \hbox {$\epsilon $-machine}\ for the Restricted Golden Mean Process.}}{93}{figure.4.4}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Time-reversed presentation of the Restricted Golden Mean Process.}}{94}{figure.4.5}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Reverse Restricted Golden Mean Process.}}{95}{figure.4.6}
\contentsline {figure}{\numberline {4.7}{\ignorespaces The \unhbox \voidb@x \hbox {$\epsilon $-machine}\ for the $\infty $-cryptic Nemo Process.}}{95}{figure.4.7}
\contentsline {figure}{\numberline {4.8}{\ignorespaces The time-reversed presentation, $\mathaccentV {widetilde}365{M}^+ = \mathcal {T}( { M }^{+} )$, of the Nemo Process. }}{97}{figure.4.8}
\contentsline {figure}{\numberline {4.9}{\ignorespaces The reverse \unhbox \voidb@x \hbox {$\epsilon $-machine}\ for the Nemo Process. }}{98}{figure.4.9}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {B.1}{\ignorespaces The simplest I-diagram - one random variable, $X$.}}{105}{figure.B.1}
\contentsline {figure}{\numberline {B.2}{\ignorespaces I-diagram for two random variables, $X$ and $Y$.}}{105}{figure.B.2}
\contentsline {figure}{\numberline {B.3}{\ignorespaces The mutual information between $X$ and $Y$ is highlighted.}}{106}{figure.B.3}
\contentsline {figure}{\numberline {B.4}{\ignorespaces The conditional entropy of $X$ given $Y$ is highlighted.}}{106}{figure.B.4}
\contentsline {figure}{\numberline {B.5}{\ignorespaces The conditional entropy of $X$ given $Y$ and $Z$ is highlighted.}}{107}{figure.B.5}
\contentsline {figure}{\numberline {B.6}{\ignorespaces The mutual information of $X$ and joint variable $W$ is highlighted.}}{108}{figure.B.6}
\contentsline {figure}{\numberline {B.7}{\ignorespaces The conditional mutual entropy of $X$ and $Y$ given $Z$ is highlighted.}}{108}{figure.B.7}
\contentsline {figure}{\numberline {B.8}{\ignorespaces The standard stratification of the conglomerate random variable $\Past $.}}{109}{figure.B.8}
\addvspace {10\p@ }
\addvspace {10\p@ }
